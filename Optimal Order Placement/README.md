# Overview
I use `@dataclass` objects. `VenueLevel` capture the best quote at one venue (price and sizing) while `Snapshot` groups multiple `VenueLevel`(s) under a timestamp. Data ingestion is handled by `load_snapshots()` which sorts the events based on the time then removed duplicates. 

Split enumration finds all the possible `n` tuples of share counts (in multiples of `chunk` defaulted to 100) that sums to the remaining order size. Using meshgrid and reshape evaluate candidates fast in C. The matrix is then feed into `compute_cost_matrix` for a one pass cost evaluation. The cost function is grabbed from the Cont-Kukanov paper. `allocate` first tries the grid if no split matches (which is rare but possible when the depth/sizing is thin and chunk is larger). If that fails it falls back to a strategy where all shares are sent to the cheapest venue. `run_backtest` simply loops through snapshots calling `allocate` and record the cumnlative spend. Baselines use the three mentioned in the document. 

For searching. I used a random log uniform search over four orders of magnitude in the 3 paramters. A regular grid search would waster 90% of trials on irrelevant scales whil log-uniform gives ever order of magnitude equal probability. Because hte cost evaluation was vectorised, adding more trial is trivial. 

Potential Improvements: The two lines that keep the allocator safe allows the strategy to fall back on `hit best ask` which is why the `best_ask` baseline is similar if not the same as the static allocator. If the best venue is deep and penalties are low, it will default to the best ask strategy. I can lower chunks, widen search range or inject a price-impact term that makes sweeping expensive. Additionally, I would replace random search with Bayesian optimisation for faster convergence. Finally, replaying the tape in the sell direction—or on a thinner-liquidity symbol—would give a more realistic view of the router’s edge. 